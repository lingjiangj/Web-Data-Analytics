{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_twitter_sentiment():\n",
    "    keyword = input(\"The keyword is:\")\n",
    "    # Obtain the most recent 50 tweets about a specific keyword.\n",
    "    import time\n",
    "    from tweepy import API\n",
    "    from tweepy import Stream\n",
    "    from tweepy import OAuthHandler\n",
    "    from tweepy.streaming import StreamListener\n",
    "    import json\n",
    "\n",
    "    ckey = 'fill in your own key'\n",
    "    csecret = 'fill in your own key'\n",
    "    atoken = 'fill in your own key'\n",
    "    asecret = 'fill in your own key'\n",
    "\n",
    "    class listener(StreamListener):\n",
    "    \n",
    "        counter = 0\n",
    "\n",
    "        def on_data(self, data):\n",
    "            listener.counter += 1\n",
    "            print(str(listener.counter)+'.' +'Got the Some Tweets for you. Press stop when you have enough.\\n')\n",
    "       \n",
    "            try:\n",
    "                outFile = open('Files_Directory/Twitter/twitterResults.txt','a')\n",
    "                outFile.write(data)\n",
    "                outFile.write('\\n')\n",
    "                outFile.close\n",
    "            \n",
    "            except BaseException as e:\n",
    "                print('failed ondata, ', str(e))\n",
    "                time.sleep(5)\n",
    "        \n",
    "            # set the limit of twitters obtained to 50\n",
    "            if listener.counter < 50:\n",
    "                return True\n",
    "            else:\n",
    "                print('Max num reached = ' + str(listener.counter))\n",
    "                return False\n",
    "        \n",
    "        def on_error(self, status):\n",
    "            print(\"failed with \" + status)\n",
    "\n",
    "    auth = OAuthHandler(ckey, csecret)\n",
    "    auth.set_access_token(atoken , asecret)\n",
    "\n",
    "    twitterStream = Stream(auth, listener() )\n",
    "    twitterStream.filter(track=[keyword]) # this were we provide the argument for receving tweets\n",
    "\n",
    "    \n",
    "    # obtain the sentiment of each tweet\n",
    "\n",
    "    file = open(\"Files_Directory/Twitter/twitterResults.txt\",\"r\")\n",
    "    tweets = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    tweets_text = []\n",
    "    \n",
    "    for i in range(0,100,2):\n",
    "        tmp = json.loads(tweets[i])\n",
    "        tweets_text.append(tmp['text'])\n",
    "\n",
    "    # print(tweets_text)\n",
    "\n",
    "    #Microsoft Azure text analytics \n",
    "    yourKey = \"fill in your own key\"\n",
    "    import requests\n",
    "    import time\n",
    "    azure_sentiments = []\n",
    "    review_number = 1\n",
    "    time_counter = time.time()\n",
    "    for twitter in list(tweets_text[0:100]):\n",
    "        if review_number % 100 == 0:\n",
    "            print(\"waiting\")\n",
    "            print(abs(75 - (time.time() - time_counter))) # use commercial service to get data, need to follow the rule\n",
    "            time.sleep(abs(75 - (time.time() - time_counter))) # here we can only use 100 test reviews\n",
    "            time_counter = time.time()\n",
    "        # print(review_number)\n",
    "        review_number+=1\n",
    "        text_analytics_base_url = \"https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/\"\n",
    "    \n",
    "        sentiment_api_url = text_analytics_base_url + \"sentiment\"\n",
    "    \n",
    "        documents = {'documents' : [\n",
    "          {'id': '1', 'language': 'en', 'text': twitter}]}\n",
    "    \n",
    "        headers   = {\"Ocp-Apim-Subscription-Key\": yourKey}\n",
    "        response  = requests.post(sentiment_api_url, headers=headers, json=documents)\n",
    "        sentiments = response.json()\n",
    "        print(sentiments)\n",
    "        azure_sentiments.append(sentiments['documents'][0]['score'])\n",
    "    \n",
    "    \n",
    "    return sum(azure_sentiments)/len(azure_sentiments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_twitter_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
