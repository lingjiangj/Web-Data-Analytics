{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW#3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So far, we have collected CIKs for each of the Mutual Funds, then looked up the links of all the 13F HRs and the Information Tables, and identified them either text tables or xml tables. In class, we  obtained the information from the xml tables.  In this assignment we will obtain information from the text file.  These are not as nicely structured as the xml output.   In a csv (HW_Mutual_Fund_Info_Table_Link.csv) you will find a few of these links.  Your goal for this part of the homework is to obtain the link to the text files from the attached file  (HW_Mutual_Fund_Info_Table_Link.csv).  Then you will write a code that will go to the links of all the linked text files, and extract some columns.  Do not use Beutiful Soup for this assignment.   We provide some initial code to guide your initial steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we have to collect the links of the text Info Tables in lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "xml_link = []\n",
    "xml_Name = []\n",
    "xml_date = []\n",
    "text_link = []\n",
    "text_Name = []\n",
    "text_date = []\n",
    "\n",
    "input_file = open('HW_Mutual_Fund_Info_Table_Link.csv', 'r')\n",
    "\n",
    "rows = input_file.readlines()\n",
    "input_file.close()\n",
    "\n",
    "# Your Code on Enumerating the Lists. \n",
    "\n",
    "for row in range(len(rows)):\n",
    "    new = rows[row].split(\",\")\n",
    "    if new[4] == \"xml\":\n",
    "        xml_link.append(new[3])\n",
    "        xml_Name.append(new[1])\n",
    "        xml_date.append(new[2])\n",
    "    elif new[4] == 'text':\n",
    "        text_link.append(new[3])\n",
    "        text_Name.append(new[1])\n",
    "        text_date.append(new[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep only the links that correspond to a date after 2010 (Don't inlude 2010, start at 2011).  Hint: you can use the datetime library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#Use the following list to store the filtered values.\n",
    "filtered_dates = []\n",
    "filtered_name = []\n",
    "filtered_link = []\n",
    "\n",
    "#Enter code here to to keep only the dates corresponding to after 2010.  \n",
    "for row in range(len(xml_date)):\n",
    "    if datetime.strptime(xml_date[row],'%Y-%m-%d') >= datetime(2011,1,1):\n",
    "        filtered_dates.append(xml_date[row])\n",
    "        filtered_name.append(xml_Name[row]) \n",
    "        filtered_link.append(xml_link[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, for each text link, extract the name of issuer, CUSIP, and the Quantity of shares.  You will also want to keep track of the mutual fund name as well as the filing report date.  \n",
    "\n",
    "#### Your output file should have 5 columns.  The first is the issue date of the form which can be found in the filtered_date list (this will be repeated for the same form).  The second is the mutual fund name which can be found in the filtered_name list (this will be repeated).  The third, fourth and fifith are the name of issuer, CUSIP, and shares respectively.  Make sure to account for the fact that while some of the text files have the same formatting, others do not.  This means you will have to look through them to make sure your code works for the each text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2830"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import codecs\n",
    "\n",
    "issue_date = []\n",
    "mutual_fund_name = [] \n",
    "name_of_issuer = [] \n",
    "CUSIP = [] # CUSIP number\n",
    "shares = [] # No. of Shares of the company in the Mutual Fund\n",
    "\n",
    "for link in range(len(filtered_link)):  \n",
    "    url = 'https://www.sec.gov'+filtered_link[link]\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "\n",
    "    soup = bs.BeautifulSoup(html,\"html.parser\")\n",
    "    \n",
    "    infotable = soup.find_all(\"table\", summary = \"Form 13F-NT Header Information\")\n",
    "    \n",
    "    for row in infotable[0].find_all('tr')[3:]:\n",
    "        columns = row.find_all('td')\n",
    "        name_of_issuer.append(columns[0].getText())\n",
    "        CUSIP.append(columns[2].getText())\n",
    "        shares.append(columns[4].getText())\n",
    "        mutual_fund_name.append(filtered_name[link])\n",
    "        issue_date.append(filtered_dates[link])\n",
    "len(name_of_issuer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#Use the following list to store the filtered values.\n",
    "new_dates = []\n",
    "new_name = []\n",
    "new_link = []\n",
    "\n",
    "#Enter code here to to keep only the dates corresponding to after 2010.  \n",
    "for row in range(len(text_date)):\n",
    "    if datetime.strptime(text_date[row],'%Y-%m-%d') >= datetime(2011,1,1):\n",
    "        new_dates.append(text_date[row])\n",
    "        new_name.append(text_Name[row]) \n",
    "        new_link.append(text_link[row]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in range(len(new_link)):\n",
    "    url = 'https://www.sec.gov'+new_link[link]\n",
    "    html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    \n",
    "    while html.find(\"<CAPTION>\") != -1:\n",
    "        index = html.find(\"<CAPTION>\") \n",
    "        html2 = html[index+10:]\n",
    "        index_Start = html2.find(\"<\")\n",
    "        index_End = html2.find(\"</TABLE>\")\n",
    "        \n",
    "        if html2.find(\"GRAND\")!= -1:\n",
    "            index_End = html2.find(\"GRAND\")\n",
    "            html2 = html2[index_Start:index_End]\n",
    "        else:\n",
    "            html2 = html2[index_Start:index_End]  #going closer to the results table\n",
    "   \n",
    "        findIndex = html2.split(\"\\n\")\n",
    "        #Find Issuer Name Index for each line\n",
    "        NameIndex_Start = findIndex[0].find(\"<\")\n",
    "        NameIndex_End = findIndex[0][NameIndex_Start+1:].find(\"<\")+1\n",
    "        #Find CUSIP Index for each line\n",
    "        CUSIP_Start = findIndex[0].find(\"<\",NameIndex_End+1)\n",
    "        CUSIP_End = findIndex[0].find(\"<\",CUSIP_Start+1)\n",
    "        #Fine Share Index for each line\n",
    "        Share_Start = findIndex[0].find(\"<\",CUSIP_End+1)\n",
    "        Share_End = findIndex[0].find(\"<\",Share_Start+1)\n",
    "    \n",
    "        # for file1 and file7, we have different startlines to get data.\n",
    "        if link == 0:\n",
    "            startline = 3\n",
    "        elif link == 6:\n",
    "            startline = 4\n",
    "        else:\n",
    "            startline = 1\n",
    "    \n",
    "        for row in range(startline,len(findIndex)):\n",
    "            # remove blank items, if the issuer name is seperated in 2 lines, add the second line to the previous line.\n",
    "            if findIndex[row][NameIndex_Start:NameIndex_End].strip() != '':\n",
    "                if findIndex[row][CUSIP_Start:CUSIP_End]!='':\n",
    "                    issue_date.append(new_dates[link])\n",
    "                    mutual_fund_name.append(new_name[link])\n",
    "                    name_of_issuer.append(findIndex[row][NameIndex_Start:NameIndex_End].strip())\n",
    "                    CUSIP.append(findIndex[row][CUSIP_Start:CUSIP_End].strip())\n",
    "                    shares.append(findIndex[row][Share_Start:Share_End].strip())\n",
    "                else:\n",
    "                    name_of_issuer[-1] = name_of_issuer[-1]+findIndex[row][NameIndex_Start:NameIndex_End].strip()\n",
    "        html = html[index_End:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, you write down the lists in the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import pandas as pd\n",
    "\n",
    "result = pd.DataFrame({\"Issue_date\":issue_date,\"Name\":mutual_fund_name,\"Name_of_Issuer\":name_of_issuer,\"CUSIP\":CUSIP,\"Shares\":shares})\n",
    "\n",
    "result.to_csv(\"Mutual_Fund_Info_Table_pandas.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
